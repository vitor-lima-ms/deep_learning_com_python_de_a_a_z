{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ec4d8d-1d24-4d2d-9ca1-41ef2ffc67db",
   "metadata": {},
   "source": [
    "# Redes multicamada\n",
    "- Redes multicamadas apresentam camadas ocultas, isto é, camadas entre a entrada e a saída\n",
    "- Na camada escondinda, também há a aplicação da função soma e de ativação\n",
    "- Cada um dos neurônios da camada de entrada é conectado a cada um dos neurônios da camada oculta, e todos os neurônios da camada oculta são conectados à saída\n",
    "## Uma das funções de ativação mais utilizada\n",
    "### y = 1 / (1 + (e^(-x))) [Função sigmoide]\n",
    "- São retornado valores apenas entre 0 e 1\n",
    "- Se o valor de x for alto, o retorno será aproximadamente 1\n",
    "- Se o valor de x for pequeno, o retorno será aproximadamente 0\n",
    "- Ela não retorna valores negativos\n",
    "## Cálculo do erro\n",
    "- erro = respostaCorreta - respostaCalculada\n",
    "- erro total = (somatório dos erros) / (número de registros)\n",
    "- O valor absoluto do erro é o que importa\n",
    "- A tendência é que o erro diminua a cada atualização dos pesos. Devemos escolher um número de épocas para que a rede seja executada, sendo que em cada época, os pesos são atualizados\n",
    "### Mean Squared Error (MSE)\n",
    "- Somatório do quadrado da diferença entre o valor previsto e o valor real, dividido pelo número de registros\n",
    "- Erros maiores são penalizados, devido à potenciação\n",
    "- Facilita a identificação dos registros mais críticos pela rede neural\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "- Similar ao MSE, mas com a operação de raiz quadrada ao final do processo\n",
    "## Descida do gradiente\n",
    "- Técnica utilizada para atualizarmos os pesos com objetivo de diminuir o valor do erro\n",
    "- Calcular a derivada parcial para mover para a direção do gradiente\n",
    "- min C(w1, w2, ..., wn). Menor custo baseado nos valores dos pesos\n",
    "- O objetivo sempre será atingir o mínimo global do gráfico que representa determinado peso\n",
    "### Batch gradient descent\n",
    "- Calculamos os erros para todos os registros e depois atualizamos os pesos\n",
    "#### Mini batch gradient descent\n",
    "- Escolhe um número de registros para rodar e atualizar os pesos\n",
    "### Stochastic gradient descent\n",
    "- Calculamos o erro para um registro e atualizamos o seu peso, antes de iniciar o processo para outro registro\n",
    "- Ajuda a previnir mínimos locais\n",
    "- Mais rápido (não precisa carregar todos os registros na memória)\n",
    "### Cálculo do delta\n",
    "- Precisamos calcular o parâmetro delta após obter a derivada da função sigmoide\n",
    "- deltaSaida = erro * derivadaSigmoide\n",
    "- deltaEscondida = derivadaSigmoide * peso * deltaSaida\n",
    "### Atualização dos pesos (Backpropagation)\n",
    "- O nome desse algoritmo remete à forma como é aplicado, isto é, da saída para entrada\n",
    "- peso(n+1) = ((peso(n) * momento) + (valorEscondida * deltaSaida * taxa de aprendizagem). Atualização dos pesos da camada oculta\n",
    "- peso(n+1) = ((peso(n) * momento) + (valorEntrada * deltaEscondida * taxa de aprendizagem). Atualização dos pesos dos neurônios de entrada\n",
    "## Unidade de Bias (https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)\n",
    "- Adiçao de unidades ligadas aos neurônios da camada escondida e à saída. Possui pesos. Pode alterar os resultados. Usado de certa forma para melhorar os resultados\n",
    "## Outros parâmetros\n",
    "### Learning rate\n",
    "- O quão rápido o algoritmo vai fazer a atualização dos pesos. Em geral, fica em torno de 0.0001\n",
    "### Batch size\n",
    "- De quantos em quantos registros será feita a atualização dos pesos\n",
    "### Epochs\n",
    "- Quantas vezes os pesos devem ser atualizados\n",
    "## Funções de ativação\n",
    "### Função step\n",
    "- Retorna 0 ou 1. Utilizada na camada final, como resposta\n",
    "- Utilizada em problemas linearmente separáveis (e.g., AND, OR)\n",
    "### Função sigmoide\n",
    "- Retorna valores entre 0 e 1. Utilizada na camada final, como resposta\n",
    "- Muito utilizada em classificação binária. Retorna como se fosse uma probabilidade. Quanto mais próximo de 1, maiores as chances da entrada ser da classe de uma classe. Quanto mais próximo de 0, maiores as chances da entrada ser da outra classe\n",
    "### Função tangente hiperbólica\n",
    "- Retorna valores entre -1 e 1\n",
    "- Mais aplicada a problemas complexos, como problemas que envolvem visão computacional (e.g., autoencoders)\n",
    "### Função ReLU (Rectified Linear Units)\n",
    "- Uma das mais utilizadas em projetos complexos de Deep Learning, especificamente nas camadas ocultas. Não é utilizada na camada de saída\n",
    "- Não apresenta valor máximo. O valor enviado, se positivo, é retornado. Se negativo, 0 é retornado\n",
    "### Função softmax\n",
    "- Similar à sigmoide, mas é utilizado em problemas com mais de duas classes\n",
    "- Utilizada na camada de saída\n",
    "### Linear function\n",
    "- Utilizada em problemas de regressão\n",
    "- Retorna valores positivos e negativos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
